{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTK Data Extraction\n",
    "\n",
    "**Requirements:**\n",
    "- RTKLib must be installed\n",
    "- Run the first 3 cells and fill data into the created directories before starting\n",
    "\n",
    "**Known Issues:**\n",
    "- Rolls around 1200 UTC may get split across files.\n",
    "- Timestamps are displayed in UTC+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "### This should be the only part of the file that needs editing\n",
    "\n",
    "BUGGIES = [\"rover\"]\n",
    "\n",
    "# YYYY-MM-DD\n",
    "DATE = \"YYYY-MM-DD\"\n",
    "\n",
    "CLEANUP = False\n",
    "DEBUG = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe 0: Setup\n",
    "\n",
    "This will create all the directories needed for analysis. Please make sure to fill out the buggies and dates above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import os\n",
    "import csv\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "from pyubx2 import UBXReader\n",
    "import seaborn as sns\n",
    "from scipy.interpolate import Akima1DInterpolator, CubicSpline, splrep, splprep, splev, BSpline\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import *\n",
    "from world import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Firgure out what these args do and utilize them if helpful\n",
    "rnx2rtkpArgs = [\"-m 5\"] #, \"-i -d 9 -v 2\", \"-d 9 -v 2\"]\n",
    "\n",
    "BASESTATION_NAME = \"basestation\"\n",
    "ROOT_DATA_DIR = \"rolls_data\"\n",
    "BASESTATION_DIR = os.path.join(ROOT_DATA_DIR, f\"{BASESTATION_NAME}_data\")\n",
    "\n",
    "RAW_DATA_DIR = \"raw_ubx\"\n",
    "DECODE_DIR = \".decoded_ubx\"\n",
    "CSV_DIR = \".csv\"\n",
    "PROCESSED_CSV_DIR = \"calc_csv\"\n",
    "GRAPHS_DIR = \"graphs\"\n",
    "\n",
    "if not os.path.exists(ROOT_DATA_DIR):\n",
    "    os.mkdir(ROOT_DATA_DIR)\n",
    "    os.mkdir(BASESTATION_DIR)\n",
    "\n",
    "BASESTATION_DATE_DIR = os.path.join(BASESTATION_DIR, DATE)\n",
    "if not os.path.exists(BASESTATION_DATE_DIR):\n",
    "    os.mkdir(BASESTATION_DATE_DIR)\n",
    "\n",
    "buggy_dirs = []\n",
    "for buggy in BUGGIES:\n",
    "    buggy_dir = os.path.join(ROOT_DATA_DIR, buggy.lower())\n",
    "    if not os.path.exists(buggy_dir):\n",
    "        os.mkdir(buggy_dir)\n",
    "\n",
    "    roll_dir = os.path.join(buggy_dir, DATE)\n",
    "    if not os.path.exists(roll_dir):\n",
    "        os.mkdir(roll_dir)\n",
    "\n",
    "    intake = os.path.join(roll_dir, RAW_DATA_DIR)\n",
    "    if not os.path.exists(intake): \n",
    "        os.mkdir(intake)\n",
    "\n",
    "    decode = os.path.join(roll_dir, DECODE_DIR)\n",
    "    if not os.path.exists(decode): os.mkdir(decode)\n",
    "\n",
    "    raw_csv = os.path.join(roll_dir, CSV_DIR)\n",
    "    if not os.path.exists(raw_csv): os.mkdir(raw_csv)\n",
    "\n",
    "    processed_csv = os.path.join(roll_dir, PROCESSED_CSV_DIR)\n",
    "    if not os.path.exists(processed_csv): os.mkdir(processed_csv)\n",
    "\n",
    "    graphs = os.path.join(roll_dir, GRAPHS_DIR)\n",
    "    if not os.path.exists(graphs): os.mkdir(graphs)\n",
    "\n",
    "    buggy_dirs.append(roll_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needs_data = False\n",
    "\n",
    "if not os.listdir(BASESTATION_DATE_DIR):\n",
    "    print(f\"Please fill {BASESTATION_DATE_DIR} with basestation data!\")\n",
    "    needs_data = True\n",
    "\n",
    "for roll_dir in buggy_dirs:\n",
    "    intake = os.path.join(roll_dir, RAW_DATA_DIR)\n",
    "    if not os.listdir(intake): \n",
    "        print(f\"Please fill {intake} with roll data!\")\n",
    "        needs_data = True\n",
    "\n",
    "# Little check to stop the program if data is missing\n",
    "# Continue after adding data\n",
    "if needs_data:\n",
    "    raise FileNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rover_dir in buggy_dirs:\n",
    "    intake_dir = os.path.join(rover_dir, RAW_DATA_DIR)\n",
    "\n",
    "    # Make sure everything has a .ubx extension (for convbin)\n",
    "    bin_files = [os.path.join(intake_dir, f) for f in os.listdir(intake_dir) if f.endswith(\".BIN\")]\n",
    "    for file in bin_files:\n",
    "        prefix, _ = os.path.splitext(file)\n",
    "        new_name = prefix + \".ubx\"\n",
    "        os.rename(src=file, dst=new_name)\n",
    "    \n",
    "    rover_ubx = sorted([f for f in os.listdir(intake_dir) if f.endswith(\".ubx\")])\n",
    "    if DEBUG: print(rover_dir, \":\", rover_ubx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipe 1: Postprocessing\n",
    "\n",
    "`.ubx` --> `.pos`, `.pos.stat`, and `_events.pos` files.\n",
    "\n",
    "This notebook contains scripts for obtaining RTK solutions through various configurations and validating/assessing the quality of those solutions.\n",
    "\n",
    "`convbin`:\n",
    "Input files are `base.ubx` and `rover.ubx`.\n",
    "Generates `.obs`, `.nav`, and `.sbs` files.\n",
    "\n",
    "`rnx2rtkp`:\n",
    "Input files are `rover.obs`, `base.obs`, and `rover.nav`.\n",
    "Generates `.pos`, `.pos.stat`, and `_events.pos` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basestation_files = [file for file in os.listdir(BASESTATION_DATE_DIR) if file.endswith(\".ubx\")]\n",
    "if not basestation_files:\n",
    "    # Remember to add and/or unzip the basestation files!\n",
    "    raise FileNotFoundError\n",
    "    \n",
    "for i, base_file in enumerate(basestation_files):\n",
    "    date, time, ext = base_file.split('_')\n",
    "    \n",
    "    if date != DATE:\n",
    "        print(\"Found file with wrong date:\", base_file)\n",
    "        continue\n",
    "\n",
    "    if DEBUG: print(base_file)\n",
    "    \n",
    "    base_full_path = os.path.join(BASESTATION_DATE_DIR, base_file)\n",
    "    base_obs = os.path.join(BASESTATION_DATE_DIR, f\"{BASESTATION_NAME}-{time}.obs\")\n",
    "    base_nav = os.path.join(BASESTATION_DATE_DIR, f\"{BASESTATION_NAME}-{time}.nav\")\n",
    "    base_sbs = os.path.join(BASESTATION_DATE_DIR, f\"{BASESTATION_NAME}-{time}.sbs\")\n",
    "\n",
    "    if os.path.exists(base_obs):\n",
    "        print(\"obs file\", base_obs, \"already exists. Skipping\")\n",
    "        continue\n",
    "\n",
    "    subprocess.run([\"convbin\", base_full_path, \"-o\", base_obs, \"-n\", base_nav, \"-s\", base_sbs])\n",
    "    # os.system(f\"convbin {base_full_path} -o {base_obs} -n {base_nav} -s {base_sbs}\")\n",
    "\n",
    "    # Sanity check files are good\n",
    "    if os.path.getsize(base_nav) < 1000 or os.path.getsize(base_obs) < 10000:\n",
    "        raise Exception(f\"Base station file {base_full_path} is borked.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_rolls = []\n",
    "\n",
    "for rover_dir in buggy_dirs:\n",
    "    intake_dir = os.path.join(rover_dir, RAW_DATA_DIR)\n",
    "    decode_dir = os.path.join(rover_dir, DECODE_DIR)\n",
    "    rover_ubx = sorted([f for f in os.listdir(intake_dir) if f.endswith(\".ubx\")])\n",
    "\n",
    "    for file in rover_ubx:\n",
    "        if DEBUG: print(rover_dir, file)\n",
    "        roll_no = ''.join([n for n in os.path.basename(file) if n.isdigit()])\n",
    "        obs = os.path.join(decode_dir, f\"{roll_no:04}.obs\")\n",
    "        nav = os.path.join(decode_dir, f\"{roll_no:04}.nav\")\n",
    "        sbs = os.path.join(decode_dir, f\"{roll_no:04}.sbs\")\n",
    "\n",
    "        if os.path.exists(obs):\n",
    "            print(\"obs file\", obs, \"already exists. Skipping\")\n",
    "            valid_rolls.append((rover_dir, obs, nav))\n",
    "            continue\n",
    "\n",
    "        raw_ubx_path = os.path.join(intake_dir, file)\n",
    "        # os.system(f\"convbin {file} -o {obs} -n {nav} -s {sbs}\")\n",
    "        subprocess.run([\"convbin\", raw_ubx_path, \"-o\", obs, \"-n\", nav, \"-s\", sbs])\n",
    "\n",
    "        # Sanity check files are good\n",
    "        if not os.path.exists(obs):\n",
    "            print(f\"Files not created from {raw_ubx_path}\")\n",
    "            continue\n",
    "\n",
    "        obs_size = os.path.getsize(obs)\n",
    "        nav_size = os.path.getsize(nav)\n",
    "        if obs_size < 5000 or nav_size < 1000:\n",
    "            print(\"Obs Size:\", obs_size)\n",
    "            print(\"Nav Size:\", nav_size)\n",
    "            raise Exception(f\"convbin borked. Originating from {raw_ubx_path}\")\n",
    "\n",
    "        valid_rolls.append((rover_dir, roll_no))\n",
    "if DEBUG: print(valid_rolls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pos_files = []\n",
    "\n",
    "for roll in valid_rolls:\n",
    "    if DEBUG: print(roll)\n",
    "    obs = os.path.join(roll[0], DECODE_DIR, roll[1] + \".obs\")\n",
    "    nav = os.path.join(roll[0], DECODE_DIR, roll[1] + \".nav\")\n",
    "\n",
    "    for i, base_obs in enumerate(sorted([os.path.join(BASESTATION_DATE_DIR, f) for f in os.listdir(BASESTATION_DATE_DIR) if f.endswith(\".obs\")])):\n",
    "        for j, arg in enumerate(rnx2rtkpArgs):\n",
    "            output_file = f\"{roll[1]:04}-{i}-{j}.pos\"\n",
    "            out_file_path = os.path.join(roll[0], DECODE_DIR, output_file)\n",
    "            os.system(f\"rnx2rtkp -o {out_file_path} {arg} -p 2 -y 2 -sys G,R,E,C -t -s , -l 40.442403483 -79.946996633 286.257 {obs} {base_obs} {nav}\")\n",
    "            \n",
    "            if os.path.exists(out_file_path) and os.path.getsize(out_file_path) > 2000:\n",
    "                print(f\"Created output file {output_file}\")\n",
    "                good_pos_files.append((roll[0], output_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipe 2: Parsing\n",
    "\n",
    "This notebook parses `.pos` files output by the previous pipe, and generates `.csv` files with headers.\n",
    "**No math is involved.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posToCsv(inputPath, outputPath):\n",
    "    with open(inputPath, 'r') as file:\n",
    "        lines = [line for line in file.readlines() if not line.startswith(\"%\")] # removes lines beginning with %\n",
    "        lines = [line.strip() for line in lines] # remove leading and trailing whitespace from each line\n",
    "        lines = [[value.strip() for value in line.split(\",\")] for line in lines] # removes leading and trailing whitespace from each CSV value in each line\n",
    "\n",
    "    rows = []\n",
    "    for line in lines:\n",
    "        row = {}\n",
    "        for i in range(len(line)):\n",
    "            row[PARSE_CSV_HEADER[i]] = line[i]\n",
    "        rows.append(row)\n",
    "    \n",
    "    with open(outputPath, 'w') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=PARSE_CSV_HEADER)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: print(good_pos_files)\n",
    "\n",
    "for dir, file in good_pos_files:\n",
    "    input_path = os.path.join(dir, DECODE_DIR, file)\n",
    "\n",
    "    prefix, _ = os.path.splitext(file)\n",
    "    buggy = os.path.basename(os.path.dirname(dir))\n",
    "    new_name = '_'.join([buggy, DATE, prefix]) + \".csv\"\n",
    "    output_path = os.path.join(dir, CSV_DIR, new_name)\n",
    "    \n",
    "    posToCsv(input_path, output_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipe 3: Analysis\n",
    "\n",
    "This notebook contains scripts for calculating kinematic data.\n",
    "\n",
    "Input files must be `.csv` files generated by `rtklib-pos-to-csv.ipynb`.\n",
    "Its headers are defined in the code below.\n",
    "\n",
    "Outputs another `.csv` file _with the **different** headers_.\n",
    "Those new headers are also defined in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP_FORMAT = \"%Y/%m/%d %H:%M:%S.%f\"\n",
    "TIME_OFFSET = dt.timedelta(hours=4)\n",
    "\n",
    "# TODO make this compatible with several levels of precesion\n",
    "# converts timestamp string to UTC value (float)\n",
    "\n",
    "def timestampToLocal(ts: str):\n",
    "    return (dt.datetime.strptime(ts, TIMESTAMP_FORMAT) - TIME_OFFSET).astimezone().strftime(\"%c\")\n",
    "\n",
    "\n",
    "def timestampToValue(ts: str):\n",
    "    return (dt.datetime.strptime(ts, TIMESTAMP_FORMAT) - TIME_OFFSET).timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMagnitude(vector):\n",
    "    return math.sqrt(math.pow(vector[0], 2) + math.pow(vector[1], 2))\n",
    "\n",
    "# Conversion factor from m/s to mph\n",
    "FREEDOM_MULTIPLIER = 2.236936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code yoinked from https://github.com/CMU-Robotics-Club/RoboBuggy2/blob/sw/20230331/rb_ws/src/buggy/scripts/auton/world.py\n",
    "\n",
    "    Abstraction for the world coordinate system\n",
    "\n",
    "    The real world uses GPS coordinates, aka latitude and longitude. However,\n",
    "    using lat/lon is bad for path planning for several reasons. First, the difference\n",
    "    in numbers would be very tiny for small distances, alluding to roundoff errors.\n",
    "    Additionally, lat/lon follows a North-East-Down coordinate system, with headings\n",
    "    in the clockwise direction. We want to use an East-North-Up coordinate system, so\n",
    "    that the heading is in the counter-clockwise direction.\n",
    "\n",
    "    We do this by converting GPS coordinates to UTM coordinates, which are in meters.\n",
    "    UTM works by dividing the world into a grid, where each grid cell has a unique\n",
    "    identifier. A UTM coordinate consists of the grid cell identifier and the \"easting\"\n",
    "    and \"northing\" within that grid cell. The easting (x) and northing (y) are in meters,\n",
    "    and are relative to the southwest corner of the grid cell.\n",
    "\n",
    "    Last, we offset the UTM coordinates to be relative to some arbitrary zero point. That\n",
    "    way, the final world coordinates are relatively close to zero, which makes debugging\n",
    "    easier.\n",
    "\n",
    "    This class provides methods to convert between GPS and world coordinates. There is\n",
    "    a version for single coordinates and a version for numpy arrays.\n",
    "\"\"\"\n",
    "\n",
    "def gps_to_world(lat, lon):\n",
    "    \"\"\"Converts GPS coordinates to world coordinates\n",
    "\n",
    "    Args:\n",
    "        lat (float): latitude\n",
    "        lon (float): longitude\n",
    "\n",
    "    Returns:\n",
    "        tuple: (x, y) in meters from some arbitrary zero point\n",
    "    \"\"\"\n",
    "    utm_coords = utm.from_latlon(lat, lon)\n",
    "    x = utm_coords[0] - World.WORLD_EAST_ZERO\n",
    "    y = utm_coords[1] - World.WORLD_NORTH_ZERO\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(inputPath, outputPath):\n",
    "    input_df = pd.read_csv(inputPath)\n",
    "\n",
    "    # Converting to linear local coordinatess\n",
    "    local_coords = []\n",
    "    for i in range(len(input_df)):\n",
    "        local_coords.append(gps_to_world(input_df[\"latitude\"][i], input_df[\"longitude\"][i]))\n",
    "\n",
    "    input_df[\"latitude_meters\"] = [coord[0] for coord in local_coords]\n",
    "    input_df[\"longitude_meters\"] = [coord[1] for coord in local_coords]\n",
    "\n",
    "\n",
    "    interpolator = Akima1DInterpolator(input_df.loc[:, \"timestamp\"].map(timestampToValue), input_df.loc[:, [\"latitude_meters\", \"longitude_meters\"]])\n",
    "    # interpolator = CubicSpline(input_df.loc[:, \"timestamp\"].map(timestampToValue), input_df.loc[:, [\"latitude\", \"longitude\"]])\n",
    "    positions = interpolator(input_df.loc[:, \"timestamp\"].map(timestampToValue), nu=0)\n",
    "    velocities = interpolator(input_df.loc[:, \"timestamp\"].map(timestampToValue), nu=1)\n",
    "    accelerations = interpolator(input_df.loc[:, \"timestamp\"].map(timestampToValue), nu=2)\n",
    "\n",
    "    output_df = pd.DataFrame(columns=CALC_CSV_HEADER)\n",
    "\n",
    "    output_df[\"timestamp\"] = input_df.loc[:, \"timestamp\"].map(timestampToValue)\n",
    "    output_df[\"timestamp_str\"] = input_df[\"timestamp\"].map(timestampToLocal)\n",
    "\n",
    "    output_df[\"fix_type\"] = input_df[\"fix-type\"]\n",
    "\n",
    "    output_df[\"latitude\"] = input_df[\"latitude\"]\n",
    "    output_df[\"longitude\"] = input_df[\"longitude\"]\n",
    "    output_df[\"altitude\"] = input_df[\"altitude\"]\n",
    "\n",
    "    output_df[\"velocity\"] = velocities.tolist()\n",
    "    output_df[\"speed\"] = [getMagnitude(vector) * FREEDOM_MULTIPLIER for vector in velocities]\n",
    "\n",
    "    output_df[\"acceleration_vector\"] = accelerations.tolist()\n",
    "    output_df[\"acceleration_magnitude\"] = [getMagnitude(vector) for vector in accelerations]\n",
    "\n",
    "    output_df = output_df[output_df[\"speed\"] < 44] # Filter out anything faster than 50mph\n",
    "\n",
    "    output_df.to_csv(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rover_dir in buggy_dirs:\n",
    "    raw_csv_dir = os.path.join(rover_dir, CSV_DIR)\n",
    "    clean_csv_files = sorted([f for f in os.listdir(raw_csv_dir) if f.endswith(\".csv\")])\n",
    "    if DEBUG: print(clean_csv_files)\n",
    "\n",
    "    for name in clean_csv_files:\n",
    "        input_path = os.path.join(rover_dir, CSV_DIR, name)\n",
    "        output_path = os.path.join(rover_dir, PROCESSED_CSV_DIR, name)\n",
    "        calc(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipe 4: Visualization\n",
    "\n",
    "Notebook contains scripts for visualizing kinematic data.\n",
    "\n",
    "Inputs files must be `.csv` file output by `analysis.ipynb`.\n",
    "\n",
    "_Note that these `.csv` files are different from those generated by `rtklib-pos-to-csv.ipynb` as they have different headers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(df, outputFilepath):\n",
    "    layers = [{\"below\": \"traces\",\n",
    "               \"sourcetype\": \"raster\",\n",
    "               \"source\": [\"https://imagery.pasda.psu.edu/arcgis/rest/services/pasda/PEMAImagery2018_2020/MapServer/WMTS/tile/1.0.0/pasda_PEMAImagery2018_2020/default/default028mm/{z}/{y}/{x}.png\"]}]\n",
    "\n",
    "    fig = px.scatter_mapbox(df,\n",
    "                            lat=\"latitude\", lon=\"longitude\",\n",
    "                            color=\"speed\",\n",
    "                            hover_data=[\"altitude\",\n",
    "                                        \"acceleration_magnitude\",\n",
    "                                        \"timestamp_str\"],\n",
    "                            zoom=15, size_max=18,\n",
    "                            height=400, width=800)\n",
    "\n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"white-bg\",\n",
    "        mapbox_layers=layers\n",
    "    )\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "    # fig.show()\n",
    "    plotly.offline.plot(fig, filename=outputFilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_speed_graph(df, outputFilepath):\n",
    "    plt.figure().set_figwidth(15)\n",
    "    plt.plot(df[\"timestamp\"], df[\"speed\"])\n",
    "    plt.savefig(outputFilepath)\n",
    "\n",
    "def plot_accel_graph(df, outputFilepath):\n",
    "    plt.figure().set_figwidth(15)\n",
    "    plt.plot(df[\"timestamp\"], df[\"acceleration_magnitude\"])\n",
    "    plt.savefig(outputFilepath)\n",
    "\n",
    "def plot_speed_accel_graph(df, outputFilepath):\n",
    "    fig, (speed, accel) = plt.subplots(2)\n",
    "    fig.set_figwidth(15)\n",
    "    speed.plot(df[\"timestamp\"], df[\"speed\"])\n",
    "    speed.set_ylim([0, 20])\n",
    "    accel.plot(df[\"timestamp\"], df[\"acceleration_magnitude\"])\n",
    "    accel.set_ylim([-1, 50])\n",
    "    plt.savefig(outputFilepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rover_dir in buggy_dirs:\n",
    "    calc_csv_dir = os.path.join(rover_dir, PROCESSED_CSV_DIR)\n",
    "    calc_files = sorted([f for f in os.listdir(calc_csv_dir) if f.endswith(\".csv\")])\n",
    "\n",
    "    for file in calc_files:\n",
    "        df = pd.read_csv(os.path.join(calc_csv_dir, file))\n",
    "        roll_name, _ = os.path.splitext(file)\n",
    "        output_path = os.path.join(rover_dir, GRAPHS_DIR)\n",
    "        plot_map(df, os.path.join(output_path, f\"{roll_name}-map.html\"))\n",
    "        # plot_speed_graph(df, os.path.join(output_path, f\"{roll_name}-speed.png\"))\n",
    "        # plot_accel_graph(df, os.path.join(output_path, f\"{roll_name}-accel.png\"))\n",
    "        # plot_speed_accel_graph(df, os.path.join(output_path, f\"{roll_name}-speed-accel.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipe 5: Cleanup\n",
    "\n",
    "If `CLEANUP` is set to true, all files except for the plots will be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANUP:\n",
    "    for buggy in BUGGIES:\n",
    "        buggy_dir = os.path.join(ROOT_DATA_DIR, buggy.lower())\n",
    "        roll_dir = os.path.join(buggy_dir, DATE)\n",
    "        \n",
    "        decode = os.path.join(roll_dir, DECODE_DIR)\n",
    "        if os.path.exists(decode):\n",
    "            for f in os.listdir(decode):\n",
    "                os.remove(os.path.join(decode, f))\n",
    "            os.rmdir(decode)\n",
    "\n",
    "        raw_csv = os.path.join(roll_dir, CSV_DIR)\n",
    "        if os.path.exists(raw_csv): \n",
    "            for f in os.listdir(raw_csv):\n",
    "                os.remove(os.path.join(raw_csv, f))\n",
    "            os.rmdir(raw_csv)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.5 ('RD25_RTK': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd2f72180070ab1630fc17d10f237a57f2f409b709228c0ebe19f0575942d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
